# -*- coding: utf-8 -*-
"""textAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RTrhh_r9IUThK7TNtkPYJNhK3KqzmlWX
"""

import pandas as pd
import requests
from nltk.corpus import stopwords 
from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import sent_tokenize
from bs4 import BeautifulSoup
import re
import nltk

nltk.download('punkt')
nltk.download("stopwords")
df = pd.read_excel('cik_list.xlsx')
df.head()

y = 'https://www.sec.gov/Archives/'
links = [y+x for x in df['SECFNAME']]

reports = []
for url in links:
    r = requests.get(url)
    data = r.text
    soup = BeautifulSoup(data, "html.parser")
    reports.append(soup.get_text())

print(f'Total {len(reports)} reports saved')

with open('StopWords_Generic.txt','r') as f:
    stop_words = f.read()

stop_words = stop_words.split('\n')
print(f'Total number of Stop Words are {len(stop_words)}')

positiveWordsFile = 'PositiveWords.txt'
with open(positiveWordsFile,'r') as posfile:
    positivewords=posfile.read().upper()
positiveWordList=positivewords.split('\n')

nagitiveWordsFile = 'NegativeWords.txt'
with open(nagitiveWordsFile ,'r') as negfile:
    negativeword=negfile.read().upper()
negativeWordList=negativeword.split('\n')

uncertainity = pd.read_excel('uncertainty_dictionary.xlsx')
uncertainity_words = list(uncertainity['Word'])

constraining = pd.read_excel('constraining_dictionary.xlsx')
constraining_words = list(constraining['Word'])

def tokenize(text):
    text = text.upper()
    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(text)
    tokenized_words = list(filter(lambda token: token not in stop_words, tokens))
    return tokenized_words

def remove_stopwords(words, stop_words):
    return [x for x in words if x not in stop_words]

def countfunc(store, words):
    score = 0
    for x in words:
        if(x in store):
            score = score+1
    return score

def sentiment(score):
    if(score < -0.5):
        return 'Most Negative'
    elif(score >= -0.5 and score < 0):
        return 'Negative'
    elif(score == 0):
        return 'Neutral'
    elif(score > 0 and score < 0.5):
        return 'Positive'
    else:
        return 'Very Positive'

def polarity(positive_score, negative_score):
    return (positive_score - negative_score)/((positive_score + negative_score)+ 0.000001)

def subjectivity(positive_score, negative_score, num_words):
    return (positive_score+negative_score)/(num_words+ 0.000001)

def syllable_2(word):
    if(len(word) > 2 and (word[-2:] == 'es' or word[-2:] == 'ed')):
        return False
    
    count =0
    vowels = ['a','e','i','o','u']
    for i in word:
        if(i.lower() in vowels):
            count = count +1
        
    if(count > 2):
        return True
    else:
        return False

def fog_index_cal(average_sentence_length, percentage_complexwords):
    return 0.4*(average_sentence_length + percentage_complexwords)

def personal_pronoun(text):
    patt=re.compile(r"\bhe|she|it+\b")
    pr= list(filter(patt.match,text))
 
    pron=[]

    for i in text:
        if i in pr:
            pron.append(i)

        else:
            continue
    return pron

new_columns=['positive_score',
      'negative_score',
      'polarity_score',
      'average_sentence_length',
      'percentage_of_complex_words',
      'fog_index',
      'complex_word_count',
      'word_count',
      'uncertainity_score',
      'constraining_score',
      'positive_word_proportion',
      'negative_word_proportion',
      'uncertainity_word_proportion',
      'constraining_word_proportion',
      'constraining_words_whole_report']
df = pd.concat([df,pd.DataFrame(columns=new_columns)])
df = df.reset_index()

df.head()

for i in range(len(reports)):
    text = re.sub('Item','ITEM',reports[i])
    tokenized_words = tokenize(text) 
    words = remove_stopwords(tokenized_words, stop_words)        
    num_words = len(words)
                
    positive_score = countfunc(positiveWordList, words)
    negative_score = countfunc(negativeWordList, words)
            
    polarity_score = polarity(positive_score, negative_score)
    #print(polarity_score)
    subjectivity_score = subjectivity(positive_score, negative_score, num_words)
    #print(subjectivity_score)
    #print(sentiment(polarity_score))
                
    sentences = sent_tokenize(text)
    num_sentences = len(sentences)
    average_sentence_length = num_words/num_sentences
    #print(average_sentence_length)
                
                
    num_complexword =0
    uncertainity_score = 0
    constraining_score = 0
                
    for word in words:
        if(syllable_2(word)):
            num_complexword = num_complexword+1
        if(word in uncertainity_words):
            uncertainity_score += 1
        if(word in constraining_words):
            constraining_score += 1
                            
    percentage_complexwords = num_complexword/num_words
            #print(percentage_complexwords)
    fog_index = fog_index_cal(average_sentence_length, percentage_complexwords)
            #print(fog_index)
                
    positive_word_proportion = positive_score/num_words
    negative_word_proportion = negative_score/num_words
    uncertainity_word_proportion = uncertainity_score/num_words
    constraining_word_proportion = constraining_score/num_words
                
                
    df.at[i,'positive_score'] = positive_score
    df.at[i,'negative_score'] = negative_score
    df.at[i,'polarity_score'] = polarity_score
    df.at[i,'average_sentence_length'] = average_sentence_length
    df.at[i,'percentage_of_complex_words'] = percentage_complexwords
    df.at[i,'fog_index'] = fog_index
    df.at[i,'complex_word_count'] = num_complexword
    df.at[i,'word_count'] = num_words
    df.at[i,'uncertainity_score'] = uncertainity_score
    df.at[i,'constraining_score'] = constraining_score
    df.at[i,'positive_word_proportion'] = positive_word_proportion
    df.at[i,'negative_word_proportion'] = negative_word_proportion
    df.at[i,'uncertainity_word_proportion'] = uncertainity_word_proportion
    df.at[i,'constraining_word_proportion'] = constraining_word_proportion
                            
    constraining_words_whole_report = 0
    tokenized_report_words = tokenize(reports[i])
    report_words = remove_stopwords(tokenized_report_words, stop_words)
    for word in report_words:
        if word in constraining_words:
            constraining_words_whole_report += 1
    #print(constraining_words_whole_report)
    df.at[i,'constraining_words_whole_report'] = constraining_words_whole_report

df.head()

df.to_excel('output.xlsx')